[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n27 min\n\n\n\nposts\n\n\nscholarly-publishing\n\n\nspanish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20 min\n\n\n\nposts\n\n\nscholarly-publishing\n\n\nenglish\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#post",
    "href": "index.html#post",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n27 min\n\n\n\nposts\n\n\nscholarly-publishing\n\n\nspanish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20 min\n\n\n\nposts\n\n\nscholarly-publishing\n\n\nenglish\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#short-stories",
    "href": "index.html#short-stories",
    "title": "Blog",
    "section": "Short Stories",
    "text": "Short Stories\n\nBrief tales that delve into the complexities of human experience, ranging from intimate daily moments to journeys into the realm of the extraordinary.\n\n\n\n\n\n\nNo matching items\n\n\n\nExplore all stories →"
  },
  {
    "objectID": "index.html#aphorisms-reflections",
    "href": "index.html#aphorisms-reflections",
    "title": "Blog",
    "section": "Aphorisms & Reflections",
    "text": "Aphorisms & Reflections\n\nBrief thoughts and reflections on life, knowledge, and human experience, updated regularly with new insights.\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n\nNo matching items\n\n\n\nRead all aphorisms →"
  },
  {
    "objectID": "posts/autorship-AI/index.html",
    "href": "posts/autorship-AI/index.html",
    "title": "Authorship and Contribution in the Age of AI: Notes on the Current Debate",
    "section": "",
    "text": "Photo by Etienne Girardet on Unsplash\nAuthorship in academic articles is not merely recognition of participation, but a key component of academic and scientific ethics. This post synthesizes international criteria and essential reflections on what it truly means to sign as an author of an academic work in the age of artificial intelligence, with the goal of contributing to outlining actions and standards for the use of AI in academic and scientific settings.\nLikewise, basic questions regarding use and implementation are raised in order to expand the critical dimension with which this topic is currently approached. Problems such as reproduction, invention, argumentative verbosity, among others, are implicit practices in various AI models, characteristic of a concatenation of data without self-awareness."
  },
  {
    "objectID": "posts/autorship-AI/index.html#what-is-authorship",
    "href": "posts/autorship-AI/index.html#what-is-authorship",
    "title": "Authorship and Contribution in the Age of AI: Notes on the Current Debate",
    "section": "What is Authorship?",
    "text": "What is Authorship?\nAccording to the Royal Spanish Academy, authorship is commonly defined as the “quality of being an author,” and an author is the person who is the “cause of something” (Real Academia Española, 2025). The word “author” comes from the Latin auctor, -óris, which meant ‘creator,’ ‘historical source,’ and ‘instigator.’ This Latin term derives from the verb augére (‘to increase,’ ‘to make progress’), so it originally designated one who made something grow or promoted it. The word has been documented in Spanish since 1155 (Corominas & Pascual, 1984) and evolved semantically from the idea of “one who makes grow” to “creator of a work.” This same Latin root generated other words like “authority,” “authorize,” and “grant,” all related to the concepts of origin, growth, and responsibility.\nFor its part, the Greek word ποιητής refers to one who “creates” or “the one who makes,” and whether that creation was by technique or by inspiration was the Platonic debate (Platón, 2022). Thus, to this day, What is authorship? or by relation, What is it to be an author and what is the condition for it? are nodal problems that precede AI and must be recognized as such. However, AI presents new dimensions and possibilities for analysis.\nFoucault (1994) argues in one of his conferences on February 22, 1969:\n\nCómo se individualizó el autor en una cultura como la nuestra, qué estatuto se le dio, a partir de qué momento, por ejemplo, empezaron a hacerse investigaciones de autenticidad y de atribución, en qué sistema de valoración quedó atrapado, en qué momento se comenzó a contar la vida ya no de los héroes sino de los autores, cómo se instauró esa categoría fundamental de la crítica: “El hombre-y-la obra”, todo esto merecería sin duda alguna ser analizado. (p. 54)\n\n“What does it matter who speaks, someone said, what does it matter who speaks” is the nodal question of the argument that Foucault takes from Beckett. Where is the author? It is difficult to find:\n\nEn la escritura no se trata de la manifestación o de la exaltación del gesto de escribir; no se trata de la sujeción de un sujeto a un lenguaje; se trata de la apertura de un espacio en donde el sujeto escritor no deja de desaparecer. (Foucault, 1994, p. 55)\n\nWondering how far an author’s work extends, Foucault reflects accurately on something that is extremely important to reconsider today: How far does an author’s work extend? If AI mediates, voluntarily or involuntarily, in the process of research, writing, creation, Who is the author?:\n\nMas supongamos que tuviéramos que ver con un autor: ¿todo lo que escribió o dijo, todo lo que dejó tras él forma parte de su obra? Problema a la vez teórico y técnico. Cuando se emprende la publicación de las obras de Nietzsche, por ejemplo, ¿en dónde hay que detenerse? Hay qué publicar todo, ciertamente, pero ¿qué quiere decir este “todo”? Todo lo que el propio Nietzschte publicó, de acuerdo. ¿Los borradores de sus obras? Ciertamente. ¿Los proyectos de aforismos? Sí. ¿También los tachones, las notas al pie de los cuadernos? Sí. Pero cuando en el interior de un cuaderno lleno de aforismos se encuentra una referencia, la indicación de una cita o de una dirección, una cuenta de la lavandería: ¿obra o no obra? ¿Y por qué no? Y esto indefinidamente. (Foucault, 1994, pp. 56–57)\n\nWork or not work? Is it legitimate to use AI in academic writing? At this point, rather: In what way is it legitimate to use AI in academic writing? Where is the limit between the author and collaboration? As we can see, this is not a current problem but one of long standing; however, today it takes on greater relevance and scope. The veil is lifted and it is easier or more accessible for more subjects to question provenance, criticize authenticity, authorship."
  },
  {
    "objectID": "posts/autorship-AI/index.html#international-criteria-icmje-and-credit",
    "href": "posts/autorship-AI/index.html#international-criteria-icmje-and-credit",
    "title": "Authorship and Contribution in the Age of AI: Notes on the Current Debate",
    "section": "International Criteria: ICMJE and CRediT",
    "text": "International Criteria: ICMJE and CRediT\n\nThe Four ICMJE Criteria\nAlready in the academic realm, at a prudent distance from the philosophical or magisterial realm of Michel Foucault’s conferences, it is simpler and necessary to cut the limits of definitions, of meanings. Thus, the International Committee of Medical Journal Editors (ICMJE) proposes four criteria that must be met simultaneously to justify authorship (International Committee of Medical Journal Editors, 2023):\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work\nDrafting the work or revising it critically for important intellectual content\nFinal approval of the version to be published\nResponsibility for all aspects of the work, ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved\n\nThese criteria establish that all those designated as authors must meet the four criteria for authorship, and all who meet the four criteria must be identified as authors. All those who do not meet the four criteria should not be listed as authors, but should be recognized as collaborators according to International Committee of Medical Journal Editors (2023).\nRegarding the use of AI, the cited document specifically states:\n\nAt submission, the journal should require authors to disclose whether they used artificial intelligence (AI)-assisted technologies (such as Large Language Models [LLMs], chatbots, or image creators) in the production of submitted work. Authors who use such technology should describe, in both the cover letter and the submitted work in the appropriate section if applicable, how they used it. For example, if AI was used for writing assistance, describe this in the acknowledgment section (see Section II.A.3). If AI was used for data collection, analysis, or figure generation, authors should describe this use in the methods (see Section IV.A.3.d). Chatbots (such as ChatGPT) should not be listed as authors because they cannot be responsible for the accuracy, integrity, and originality of the work, and these responsibilities are required for authorship (see Section II.A.1). Therefore, humans are responsible for any submitted material that included the use of AI-assisted technologies. Authors should carefully review and edit the result because AI can generate authoritative-sounding output that can be incorrect, incomplete, or biased. Authors should not list AI and AI-assisted technologies as an author or co-author, nor cite AI as an author. Authors should be able to assert that there is no plagiarism in their paper, including in text and images produced by the AI. Humans must ensure there is appropriate attribution of all quoted material, including full citations. (International Committee of Medical Journal Editors, 2023)\n\nIn this regard, two fundamental issues should be highlighted:\n\nThe reason why AI cannot be an author is because it does not meet one of the authorship criteria defined above: it is not subject to responsibilities.\nThe use of AI: it is pertinent that authors declare the use, type, and manner in which it was done.\n\nNow, regarding use, some questions arise. In times when AI is transforming search models (Microsoft, 2023), replacing traditional search engines that have dominated the internet for the last 30 years and involuntarily creeping into search results, is a simple declaration sufficient? How does one declare being within a new paradigm? AI is no longer optional, or at least for those who use digital research media, and its use will be almost inevitable. In the Kuhnian sense of the term (Kuhn, 1970), AI represents a paradigmatic change in the search and processing of scientific information. This new paradigm raises the question of whether it is possible to fully declare the use of AI when it has become ubiquitous in digital research processes.\n\n\nThe CRediT Taxonomy\nComplementing this definition, the CRediT taxonomy is a community taxonomy of 14 roles that can be used to describe the key types of contributions typically made to the production and publication of research results such as research articles. This taxonomy allows for more granular attribution of individual contributions to academic work (CRediT, 2022).\nThe 14 CRediT roles include everything from conceptualization to data curation, providing a framework for recognizing different types of contributions beyond traditional authorship and can be applied to both authors and collaborators according to the definition of International Committee of Medical Journal Editors (2023)."
  },
  {
    "objectID": "posts/autorship-AI/index.html#key-concepts-in-academic-authorship",
    "href": "posts/autorship-AI/index.html#key-concepts-in-academic-authorship",
    "title": "Authorship and Contribution in the Age of AI: Notes on the Current Debate",
    "section": "Key Concepts in Academic Authorship",
    "text": "Key Concepts in Academic Authorship\nAuthorship in scientific publications represents one of the most complex and controversial aspects of the editorial process, requiring a clear understanding of the responsibilities, hierarchies, and ethical practices involved. Returning to what was outlined in International Committee of Medical Journal Editors (2023), COPE defines a series of key concepts for academic authorship (Albert & Wager, 2003).\n\nHierarchies and Author Order\nThe order of authors conveys crucial information about relative contributions to the research work. The first author is traditionally considered the one who made the greatest contribution and is frequently the principal investigator responsible for study execution and initial manuscript writing. This position is particularly valued given that academic citations typically refer to studies by the first author’s surname followed by “et al.”\nThe position of the last author presents greater interpretative variability across disciplines; it is commonly assigned to a senior team member who provided supervision, methodological expertise, or institutional resources, although it is often suspected that this may involve honorary authorship.\n\n\nProblematic Practices in Authorship\nInappropriate behaviors related to authorship include honorary authorship (adding someone who did not contribute) and ghost authorship (omitting someone who did contribute). Both practices violate the principles of scientific integrity (Codina, 2023).\nAlbert & Wager (2003) discusses ghost authorship and honorary authorship.\nGhost authorship includes both professional writers (frequently hired by commercial sponsors) whose participation is not acknowledged, and researchers who made significant contributions but were omitted from the author list. This practice represents a potential conflict of interest and violates the principles of scientific transparency.\nHonorary authorship involves the inclusion of people who did not meet the ICMJE criteria, typically academic or administrative authority figures whose inclusion seeks to obtain political favor or institutional prestige. Another variant includes reciprocal agreements between colleagues to include each other in publications regardless of their actual contributions.\n\n\nResponsibilities and Guarantees\nRecognizing the growing specialization in contemporary research, ICMJE guidelines have introduced the concept of “guarantor” - one or more authors who assume responsibility for the complete integrity of the work from conception to final publication. This figure recognizes that it may be unreasonable to expect each author to deeply understand all technical aspects of the study (for example, for a radiologist to explain complex statistical methods), but maintains overall responsibility for the project.\nThe corresponding author, although playing an administrative role in communication with editors and readers, should not be automatically equated with academic hierarchy.\n\n\nProcedural Decisions and Acknowledgments\nDecisions about author order require explicit negotiation among collaborators, ideally before beginning manuscript writing. Some groups opt for alphabetical ordering when contributions are equivalent, a practice that should be clearly communicated to the editor.\nContributions that do not merit full authorship should be appropriately recognized in the acknowledgments, specifying the exact nature of the contribution made. Many journals frequently require signed consent from people recognized in this section.\n\n\nContemporary Implications\nThe absence of limits on the number of authors in modern databases has eliminated historical pressures to restrict lists, but the inclusion of multiple authors significantly increases the time for manuscript preparation, review, and completion.\nGroup authorship presents particular challenges in database indexing, where the first name in alphabetical lists may erroneously become the first author by default. These considerations underscore the importance of advance planning and explicit communication in collaborative research teams.\n\n\nMain Notions of Academic Authorship\nBelow is a web diagram created in HTML to synthesize the main ideas about authorship referenced above.\n\n\n\n\n\n\n\nFigure 1: Diagram: Key Concepts in Academic Authorship. Pantaleo (2025)"
  },
  {
    "objectID": "posts/autorship-AI/index.html#artificial-intelligence-and-authorship",
    "href": "posts/autorship-AI/index.html#artificial-intelligence-and-authorship",
    "title": "Authorship and Contribution in the Age of AI: Notes on the Current Debate",
    "section": "Artificial Intelligence and Authorship",
    "text": "Artificial Intelligence and Authorship\nThe authorship problem specifically related to AI deepens issues that, as mentioned, precede it. How far can AI contribute to an academic writing? How is it recognized? Given the forced use to which we are increasingly conditioned with new search models, should the use or absence of it be declared?\nA recent analysis revealed that at least 1% of scientific articles published in 2023—approximately 60,000 papers—showed signs of using language models like ChatGPT (Gray, 2024; Stokel-Walker, 2024), a figure that experts say represents merely “the tip of the iceberg” of a much broader phenomenon. Evidence of this use ranges from flagrant cases—such as the accidental inclusion of phrases like “certainly, here is a possible introduction for your topic” in papers published by Elsevier (Stokel-Walker, 2024)—to more subtle but revealing patterns in the use of scientific language. Words like “delve” experienced a 654% increase in the PubMed database between 2020 and 2023, while terms like “commendable” and “meticulous” showed similar increases (Stokel-Walker, 2024), suggesting a fundamental change in the lexicon of academic writing.\nThis trend reflects a complex reality in the contemporary academic world. In a context where researchers operate under the pressure of “publish or perish,” many turn to these tools as writing assistants or to overcome language barriers. However, what began as grammatical support is evolving toward more problematic uses: from generating scientific figures to the possible automation of the peer review process (Liang et al., 2024).\nThe phenomenon raises fundamental questions about scientific integrity, especially considering that these models are prone to “hallucinations”—inventing non-existent bibliographic references—and that automatic detectors of AI-generated content prove to be unreliable tools (Stokel-Walker, 2024), beyond the fact that many also use AI to detect AI. If it’s about judgment, should one trust without reservations a tool that uses AI to detect AI-written content? The following graphs (Figure 2) extracted from Stokel-Walker (2024) show the increased use of keywords related to AI-generated writing.\n\n\n\n\n\n\nFigure 2: Suspicious Trends in Word Usage. Amanda Montañez; Source: Andrew Gray. Stokel-Walker (2024)\n\n\n\n\nEditorial Policies on AI\nIn this context, some of the world’s major publishers have defined criteria for use and authorship related to AI. Below is a synthesis of the main editorial policies:\nCambridge University Press & Assessment (Cambridge University Press & Assessment, 2024): Establishes that AI tools must be explicitly declared and their use must be explained in detail in the manuscript. The publisher is categorical in stating that these tools do not qualify for authorship due to their fundamental inability to assume ethical, legal, and academic responsibility for the content produced.\nElsevier (Elsevier, 2024): AI tools and AI-assisted technologies do not qualify for authorship under Elsevier’s authorship policy. Authors who use these tools during the manuscript writing process must declare their use in a separate section of the manuscript, promoting transparency among authors, readers, reviewers, and editors.\nSpringer Nature (Springer Nature, 2024): Large Language Models (LLMs), such as ChatGPT, do not currently satisfy the publisher’s authorship criteria. The policy emphasizes that authorship attribution carries responsibility for the work, responsibility that cannot be effectively applied to LLMs. The use of these tools must be appropriately documented in the Methods section or in a suitable alternative part of the manuscript.\nScience Journals (Science Journals, 2023): Maintains a particularly strict policy establishing that text generated by AI, machine learning, or similar algorithmic tools cannot have authorship attribution. Violation of this policy constitutes scientific misconduct, equating it with other forms of academic fraud. It explicitly states:\n\nArtificial intelligence (AI). AI-assisted technologies [such as large language models (LLMs), chatbots, and image creators] do not meet the Science journals’ criteria for authorship and therefore may not be listed as authors or coauthors, nor may sources cited in Science journal content be authored or coauthored by AI tools. Authors who use AI-assisted technologies as components of their research study or as aids in the writing or presentation of the manuscript should note this in the cover letter and in the acknowledgments section of the manuscript. Detailed information should be provided in the methods section: The full prompt used in the production of the work, as well as the AI tool and its version, should be disclosed. Authors are accountable for the accuracy of the work and for ensuring that there is no plagiarism. They must also ensure that all sources are appropriately cited and should carefully review the work to guard against bias that may be introduced by AI. Editors may decline to move forward with manuscripts if AI is used inappropriately. Reviewers may not use AI technology in generating or writing their reviews because this could breach the confidentiality of the manuscript. (Science Journals, 2023)\n\nTaylor & Francis Group (Taylor & Francis Author Services, 2024a): AI tools cannot be considered authors under any circumstances. The publisher requires authors to appropriately document any use of these technologies, specifying what tools were used and how they contributed to the work. Taylor & Francis Group also has a specific AI policy on its website (Taylor & Francis, 2024) and guidelines on AI-assisted writing (Taylor & Francis Author Services, 2024b).\nWiley (Wiley, 2024): Establishes that AI cannot be an author due to the impossibility of assuming the responsibilities inherent to authorship. Authors must declare in detail the use of AI tools in the Methods or Acknowledgments sections, providing specific information about how these technologies were used in the research and writing process.\n\n\nConsensus on AI and Authorship\nAI tools cannot meet the requirements for authorship as they cannot assume responsibility for submitted work. As non-legal entities, they cannot assert the presence or absence of conflicts of interest nor handle copyright agreements and licenses (Committee on Publication Ethics, 2024). Authors using AI tools must be transparent in disclosing how the AI tool was used and what tool was used. Authors are fully responsible for the content of their manuscript, including those parts produced by an AI tool."
  },
  {
    "objectID": "posts/autorship-AI/index.html#best-practices-for-authorship-determination-in-relation-to-ai-use",
    "href": "posts/autorship-AI/index.html#best-practices-for-authorship-determination-in-relation-to-ai-use",
    "title": "Authorship and Contribution in the Age of AI: Notes on the Current Debate",
    "section": "Best Practices for Authorship Determination in Relation to AI Use",
    "text": "Best Practices for Authorship Determination in Relation to AI Use\n\nEarly Use Agreements\nAuthors should discuss AI use when planning research, agree on methodology and tools used in writing, and review the agreement during research. It is fundamental to establish these agreements before beginning work to avoid later disputes.\n\n\nDifferentiating Forced from Induced Use\nIt is essential that the entire research team can differentiate tools used voluntarily for conducting research or writing from those that are induced by the new web search paradigm managed by AI tools selected by corporations to show results and not by users.\n\n\nUse Declaration\nCurrently, it is recommended to introduce in articles or writings a declaration of AI use just as declarations of conflicts of interest, data availability, or funding are commonly included (Sampaio et al., 2024).\nAs AI development advances and integration with search engines progresses, this becomes essential as it not only evidences the use or non-use of some AI model in some part of the article’s elaboration but, above all, the researcher’s awareness in using the digital tools available today.\nIt is essential that people writing academic articles be attentive to differentiating search results processed by AI from those that are not. This awareness process manages to evidence AI intrusion in research processes according to the research team’s possibilities to identify it. In cases of voluntary use, clearly, defining the scope of use is crucial to identify possible fallacies or inventions in the research argumentation. But even more important is taking awareness of possible pathways for intrusion of artificial arguments or results generated by AI-conditioned searches beyond the initial intentions of the research team.\nThus, it is recommended not only to indicate when and how an AI model is used, but also when it is not used in research. Following the declaration suggested in Sampaio et al. (2024):\n\nDuring the preparation of this work, the author(s) used [name of tool/model or service] version [number and/or date] to [justify the reason]. After using this tool/model/service, the author(s) reviewed and edited the content in accordance with the scientific method and assume(s) full responsibility for the content of the publication. (p. 20)\n\nIt could also be declared when not used: “In the present work, AI models have not been used in any of its phases.”"
  },
  {
    "objectID": "posts/autorship-AI/index.html#conclusion",
    "href": "posts/autorship-AI/index.html#conclusion",
    "title": "Authorship and Contribution in the Age of AI: Notes on the Current Debate",
    "section": "Conclusion",
    "text": "Conclusion\nSigning an article is not a formality or a courtesy; it is an ethical declaration that carries legal and intellectual responsibilities. The adoption of ICMJE and CRediT frameworks, along with clear policies on AI use, ensures fair recognition of real contributions and helps prevent inappropriate behaviors that undermine scientific trust.\nIn the age of artificial intelligence, it is fundamental to maintain that authorship requires human responsibility, transparency in the use of AI tools, and adherence to established ethical criteria. The evolution toward greater multidisciplinary collaboration and responsible use of emerging technologies must be accompanied by robust ethical frameworks that protect the integrity of scientific research.\nAs Michel Foucault points out, there are discourses that are provided with authorship and others that are not. This is constructed within a society, a culture, a discourse and is validated by those who share the same argumentative guidelines:\n\nOne will finally arrive at the idea that the author’s name does not go, like the proper name, from the interior of a discourse to the real and exterior individual who produced it, but runs, in a certain way, at the limit of texts, cuts them out, follows their edges, manifests their mode of being or, at least, characterizes it. It manifests the event of a certain set of discourse, and refers to the status of this discourse within a society and within a culture. The author’s name is not situated in the civil status of men, nor is it situated in the fiction of the work; it is situated in the rupture that establishes a certain group of discourse and its singular mode of being. It could be said, therefore, that in a civilization like ours there are a certain number of discourses endowed with the “author” function while others are deprived of it. A private letter may well have a signatory, but it does not have an author; a contract may have a guarantor, but it does not have an author. An anonymous text read on the street on a wall will have a writer, but will not have an author. The author function is, then, characteristic of the mode of existence, circulation and functioning of certain discourses within a society. (Foucault, 1994, pp. 60–61)\n\nIt could be concluded that AI as such is deprived of authorship because it is not responsible nor conscious of its own creative act; it is an excellent tool. As such, its use by subjects is what constitutes an ethical act. It can be used ethically to enrich and develop new argumentative models or it can be used to develop text without any human care. In any case, the responsible act of human consciousness is what determines authorship.\n\n\n\n\n\n\nNote 1\n\n\n\nIn writing this post, AI has intervened in paragraph writing, idea synthesis, bibliography summarization, and the creation of metadata and text in MD, YAML, and BibTeX. It has also responded to queries and requests made by the author to expand criticism and improve syntax. AI assistance was provided by Claude Sonnet 4 (Anthropic).\n\n\n\n\n\n\n\n\nNote 2\n\n\n\nOriginally composed in Spanish, this article has been translated into English while preserving references to Spanish-language sources, including the Royal Spanish Academy Dictionary. To ensure scholarly accuracy, all textual citations are maintained in their original languages—Spanish, English, and Portuguese. The translation was generated using Claude Sonnet 4 (Anthropic) and subsequently reviewed and validated by the author."
  },
  {
    "objectID": "cuentos.html",
    "href": "cuentos.html",
    "title": "Short Stories",
    "section": "",
    "text": "Here you’ll find all the short stories and brief narratives, exploring different literary themes and narrative styles.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "aforismos.html",
    "href": "aforismos.html",
    "title": "Aphorisms & Reflections",
    "section": "",
    "text": "A curated collection of aphorisms, brief reflections, and thoughtful maxims exploring various aspects of human experience and wisdom.\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\nDate\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ia-descentralizada/index.html",
    "href": "posts/ia-descentralizada/index.html",
    "title": "Inteligencia artificial descentralizada (DeAI): perspectivas de un desarrollo en ciernes",
    "section": "",
    "text": "Foto de Claudio Schwarz en Unsplash\nEn tiempos donde la inteligencia artificial alborea en el escenario de lo cotidiano, donde su utilización se afianza día a día y los interrogantes de uso son cada vez mayores, se plantea aquí una guía de lectura nodal para un marco crítico que se proponga analizarla. Esta guía reside en una conceptualización básica, la de inteligencia artificial descentralizada (DeAI). A su vez, presupone una diferenciación inicial sobre la cual partir; recursos de uso centralizado y recursos de uso descentralizado. Así, este post se propone abordar las principales orientaciones de lectura para un fenómeno que promete avanzar cada vez más en la vida humana, la inteligencia artificial, y la gestión de recursos que la sustentan."
  },
  {
    "objectID": "posts/ia-descentralizada/index.html#centralización-vs.-descentralización-definiendo-el-paradigma-tecnológico",
    "href": "posts/ia-descentralizada/index.html#centralización-vs.-descentralización-definiendo-el-paradigma-tecnológico",
    "title": "Inteligencia artificial descentralizada (DeAI): perspectivas de un desarrollo en ciernes",
    "section": "Centralización vs. descentralización: definiendo el paradigma tecnológico",
    "text": "Centralización vs. descentralización: definiendo el paradigma tecnológico\nPara comprender el alcance y las implicaciones de la inteligencia artificial descentralizada, es fundamental establecer primero la distinción entre sistemas centralizados y descentralizados en el ámbito tecnológico.\nLos sistemas centralizados se caracterizan por concentrar el control, procesamiento y toma de decisiones en un punto único o en un número limitado de entidades. En este modelo, una autoridad central —típicamente una empresa o institución— posee y opera la infraestructura, controla los protocolos de acceso, y define las reglas de funcionamiento. Los usuarios dependen completamente de esta entidad central para acceder al servicio, al procesamiento de datos o a funcionalidades específicas.\nAdicionalmente, los términos de uso y privacidad suelen expresar (a veces confusamente) qué se hace con los datos y contenido que en el sistema se procesan y quién lo hace. En un sistema centralizado, una empresa o institución decide por el común de sus usuarios que aceptaron previamente las condiciones de uso.\nEsto, probablemente es a lo que estés acostumbrado si no has profundizado a consciencia en el manejo de datos en el mundo actual; esto es así porque, porque mayoritariamente en la internet vigente los sistemas centralizados tienen el poder (y la potestad) de imponerse como elección frente al usuario antes otras opciones. Por eso, la práctica consciente de elección es fundamental para acceder a otras fuentes de gestión de recursos virtuales.\nEsta predominancia de sistemas centralizados no es meramente accidental, sino el resultado de estrategias deliberadas de control de mercado y sesgos algorítmicos sistemáticos. La investigación académica ha demostrado que los motores de búsqueda dominantes perpetúan sesgos sociales existentes, donde el sesgo algorítmico es consistente con la discriminación social (Lin et al., 2023), afectando particularmente a grupos históricamente desfavorecidos. Además, la concentración monopólica en estos mercados digitales resulta contraproducente para la innovación y calidad del servicio, ya que “una plataforma monopólica dominante resulta en precios más altos y subinversión en innovaciones de mejora de calidad” (Lianos & Motchenkova, 2013). Empresas como Google mantienen su dominancia no solo por superioridad técnica, sino gastando más de $26 mil millones anuales para asegurar su posición como motor de búsqueda predeterminado (Foucart, 2025), lo que evidencia cómo el control de la configuración por defecto se convierte en una herramienta de poder que limita la elección del usuario. Esta realidad subraya la importancia crítica de desarrollar marcos alternativos que promuevan la diversidad de plataformas y la transparencia algorítmica (Mowshowitz & Kawaguchi, 2002), especialmente en contextos donde los efectos de red pueden utilizarse para crear ecosistemas más equilibrados (Schüler & Petrik, 2023).\nUn ejemplo ilustrativo de esta dinámica de poder centralizado también se puede observar en los términos de uso de plataformas de inteligencia artificial como OpenAI, donde la empresa establece unilateralmente las condiciones bajo las cuales los usuarios pueden acceder y utilizar sus servicios (OpenAI, 2025). Estos términos determinan no solo qué tipo de contenido puede procesarse, sino también cómo se utilizarán los datos de entrada del usuario, qué derechos retiene la plataforma sobre las interacciones y bajo qué circunstancias puede suspenderse o terminarse el acceso al servicio. El usuario, para acceder a estas herramientas de IA, debe aceptar en bloque estas condiciones sin posibilidad de negociación, ejemplificando cómo los sistemas centralizados ejercen control sobre millones de usuarios mediante un simple clic de aceptación que pocas veces es leído o comprendido en su totalidad. Claro está, el problema principal no está en que una empresa establezca las condiciones bajo las cuáles puede ofrecer sus servicios, sino en cuando estas prácticas empiezan a ser monopólicas, hegemónicas y poco éticas en el uso de datos.\n\n\n\n\n\n\nUso de Wayback Machine para referencias web\n\n\n\nEn algunas referencias - como OpenAI (2025) - se utiliza Wayback Machine en lugar de las URLs originales para preservar el estado específico de documentos web que cambian frecuentemente, como términos de uso y políticas de privacidad. Esto garantiza que los lectores puedan acceder exactamente al mismo contenido citado en el momento de la investigación, mejorando la verificabilidad y reproducibilidad del trabajo académico.\n\n\nPor el contrario, los sistemas descentralizados distribuyen el control y la funcionalidad entre múltiples participantes o nodos en una red. No existe una autoridad única que controle completamente el sistema; en su lugar, las decisiones se toman colectivamente, el procesamiento se distribuye y la resiliencia del sistema no depende de un punto único de fallo. La descentralización representa un paradigma fundamental que desafía los modelos tradicionales de organización jerárquica. A diferencia de los sistemas centralizados, donde existe uno o pocos puntos de control que coordinan todas las operaciones, los sistemas descentralizados distribuyen esta responsabilidad entre múltiples entidades autónomas que colaboran para lograr objetivos comunes (Lua et al., 2005). Esta distribución del poder de decisión no solo reduce la dependencia de infraestructura crítica, sino que también permite una mayor resiliencia ante fallos, ataques o intentos de censura.\nEn el contexto de las redes informáticas, un ejemplo paradigmático son las redes peer-to-peer (P2P), que representan sistemas distribuidos donde los participantes, denominados peers o nodos, actúan simultáneamente como clientes y servidores (Lua et al., 2005). Estas redes se caracterizan por formar estructuras de red superpuestas (overlay networks) que se auto-organizan sobre la infraestructura existente del protocolo de Internet. Las redes P2P van más allá de los servicios tradicionales cliente-servidor al permitir que cada peer contribuya con recursos (ancho de banda, almacenamiento, capacidad de procesamiento) mientras simultáneamente accede a los recursos proporcionados por otros peers (Lua et al., 2005). Esta simetría en los roles elimina las limitaciones de escalabilidad inherentes a los sistemas centralizados y crea un modelo de compartición de recursos más eficiente y democrático, al menos en teoría.\nExisten diferentes tipos de redes P2P según su estructura. Las redes P2P estructuradas utilizan algoritmos determinísticos para organizar los peers y colocar los datos en ubicaciones específicas, empleando técnicas como las tablas hash distribuidas (DHT) que garantizan la localización de cualquier objeto de datos en un número logarítmico de saltos (Lua et al., 2005). Por otro lado, las redes P2P no estructuradas organizan los peers en grafos aleatorios sin un control estricto sobre la colocación de datos, utilizando técnicas como flooding1 o random walks2, lo que las hace más resilientes a la entrada y salida dinámica de peers pero menos eficientes para localizar elementos raros.\nLas características fundamentales de los sistemas descentralizados incluyen múltiples dimensiones interconectadas según Lua et al. (2005):\n\nAuto-organización y dinamismo: Los nodos se unen y abandonan la red dinámicamente sin requerir configuración centralizada o aprobación de autoridades. Este proceso implica mecanismos de descubrimiento de peers, establecimiento de conexiones y mantenimiento de la topología de red de forma autónoma. La capacidad de auto-organización permite que el sistema se adapte continuamente a cambios en la disponibilidad de recursos y patrones de uso.\nSimetría de roles y reciprocidad: Cada participante puede funcionar tanto como proveedor como consumidor de recursos, eliminando la distinción tradicional cliente-servidor. Esta simetría fomenta un modelo colaborativo donde la contribución de recursos por parte de cada peer beneficia al conjunto del sistema. Sin embargo, esta característica también introduce desafíos relacionados con incentivos para la cooperación y prevención del comportamiento oportunista.\nTolerancia a fallos y resiliencia: La ausencia de puntos únicos de fallo permite que el sistema continúe operando incluso cuando algunos nodos fallen, sean atacados o abandonen la red voluntariamente. Esta resiliencia se logra mediante redundancia de datos, múltiples rutas de enrutamiento y mecanismos de recuperación distribuidos. La capacidad de mantener la funcionalidad ante fallos parciales es una ventaja crítica sobre los sistemas centralizados.\nEscalabilidad horizontal: La capacidad del sistema para crecer agregando más nodos sin degradar significativamente el rendimiento global. En teoría, cada nuevo peer que se une a la red contribuye con recursos adicionales, lo que puede mejorar la capacidad total del sistema. No obstante, esto requiere algoritmos eficientes que mantengan las propiedades de enrutamiento y búsqueda independientemente del tamaño de la red.\nResistencia a la censura y autonomía: La distribución del control dificulta el bloqueo, la manipulación o el cierre del sistema por parte de autoridades centrales o actores maliciosos. Esta característica es particularmente relevante en contextos donde la libertad de información y la privacidad son prioritarias. La ausencia de chokepoints centralizados hace que el sistema sea inherentemente más resistente a la interferencia externa.\n\nA pesar de sus ventajas, los sistemas descentralizados y las redes P2P presentan desafíos y limitaciones significativas que deben considerarse en su implementación. En primer lugar, las redes P2P no estructuradas como Gnutella enfrentaron problemas de escalabilidad debido a que los mecanismos de búsqueda basados en flooding consumen excesivo ancho de banda y generan cargas inesperadas en la red, además de que las consultas para contenido raro o poco replicado pueden fallar debido a las limitaciones del horizonte de búsqueda impuestas por el TTL (Time-To-Live) (Lua et al., 2005, p. 85). Por otro lado, aunque las redes P2P estructuradas solucionan parcialmente estos problemas mediante algoritmos determinísticos, introducen complejidad adicional y pueden experimentar alta latencia de búsqueda cuando el camino de la red superpuesta difiere significativamente del camino físico subyacente, lo que puede afectar adversamente el rendimiento de las aplicaciones (Lua et al., 2005, pp. 88-90). Adicionalmente, estos sistemas son vulnerables a diversos ataques de seguridad, incluyendo el retorno de objetos de datos incorrectos a las consultas, corrupción o denegación de acceso a réplicas de datos, suplantación de identidad para almacenar réplicas en peers ilegítimos, y ataques de colusión donde peers maliciosos colaboran para comprometer el sistema (Lua et al., 2005). Finalmente, la gestión de incentivos representa un desafío fundamental, ya que sin mecanismos adecuados para prevenir el comportamiento oportunista (free-riding), la confiabilidad y el valor del sistema pueden verse comprometidos cuando los usuarios se benefician de los recursos compartidos sin contribuir proporcionalmente (Lua et al., 2005).\nLos desafíos inherentes a las primeras redes P2P han catalizado el desarrollo de nuevas generaciones de sistemas descentralizados que incorporan mecanismos de incentivos más sofisticados y arquitecturas mejoradas. BitTorrent v2 ha evolucionado con cifrado SHA-256 y optimizaciones que hacen las descargas más rápidas y seguras, manteniéndose como el protocolo P2P dominante (Hipertextual, 2022). Por otro lado, IPFS (Sistema de Archivos Interplanetario) (Protocol Labs, 2024) representa un paradigma más avanzado que puede combinar tecnologías P2P con blockchain para crear un protocolo de hipermedia descentralizado direccionable por contenido, diseñado para hacer la web más rápida, segura y abierta (Desde Linux, 2020). Mientras tanto, las redes blockchain como Bitcoin y Ethereum han resuelto el problema fundamental de los incentivos mediante mecanismos criptoeconómicos que recompensan la participación y penalizan el comportamiento malicioso, creando sistemas de contabilidad distribuidos donde el problema del doble gasto y el free-riding son superados mediante consenso distribuido y recompensas tokenizadas (Bit2Me Academy, 2023). Estas tecnologías modernas demuestran cómo los principios de descentralización han evolucionado desde las limitaciones de las primeras redes P2P hacia ecosistemas más robustos y sostenibles.\n\nDe la tecnología a la ciencia: el movimiento descentralizado\nEsta dicotomía entre centralización y descentralización trasciende el ámbito puramente tecnológico y se extiende a dominios como la investigación científica. Como se ha analizado en trabajos previos (Pantaleo, 2024a), la ciencia también enfrenta tensiones entre modelos centralizados —controlados por grandes grupos económicos, instituciones hegemónicas y sistemas de financiamiento concentrados— y enfoques descentralizados que buscan democratizar el acceso al conocimiento, diversificar las fuentes de financiamiento, y promover la colaboración abierta.\nEl diálogo con referentes latinoamericanos en ciencia descentralizada (Pantaleo, 2024b) revela que esta transformación no es meramente técnica, sino que implica una redefinición fundamental de las relaciones de poder, los mecanismos de validación, y los criterios de calidad en la producción de conocimiento. Estas reflexiones sobre la descentralización científica proporcionan un marco conceptual valioso para entender las dinámicas similares que emergen en el desarrollo de la inteligencia artificial."
  },
  {
    "objectID": "posts/ia-descentralizada/index.html#inteligencia-artificial-descentralizada-deai",
    "href": "posts/ia-descentralizada/index.html#inteligencia-artificial-descentralizada-deai",
    "title": "Inteligencia artificial descentralizada (DeAI): perspectivas de un desarrollo en ciernes",
    "section": "Inteligencia artificial descentralizada (DeAI)",
    "text": "Inteligencia artificial descentralizada (DeAI)\nLa inteligencia artificial ha experimentado un crecimiento exponencial en los últimos años, transformando sectores desde la medicina hasta el entretenimiento. Sin embargo, este desarrollo ha estado dominado por grandes corporaciones tecnológicas que concentran el poder computacional, los datos y el control sobre los sistemas más avanzados, como ya se viene describiendo el funcionamiento de sistemas centralizados. En este contexto es pertinente plantear un paradigma alternativo: la inteligencia artificial descentralizada (DeAI).\n\n\n\n\n\n\nDeAI, DzAI o DEAI. Definir el acrónimo\n\n\n\nEn este trabajo se pueden ver distintos acrónimos que se utilizan para referir a inteligencia artificial descentralizada. Se usa DzAI (Demazeau & Müller, 1990) o DEAI (Keršič & Turkanović, 2025). Se opta aquí, sin embargo, por DeAI (Cao, 2022) para mantener consistencia con otros acrónimos actuales en el campo de las tecnologías o disciplinas descentralizadas como DeSci (Pantaleo, 2024a) o DeFi. DeAI refiere así a Decentralized (De) Artificial Intelligence (AI).\n\n\n\nRevisión histórica: de la distribución a la descentralización\nLa conceptualización de sistemas de inteligencia artificial no centralizados se remonta a fines de los años 80. Demazeau & Müller (1990) establecieron una distinción seminal entre dos paradigmas fundamentales que sentarían las bases teóricas para los desarrollos posteriores. Su trabajo diferenciaba claramente entre Distributed Artificial Intelligence (DAI) y Decentralized Artificial Intelligence (DzAI), una distinción que resulta crucial para comprender la evolución conceptual del campo.\nEl DAI tradicional se enfocaba en la solución colaborativa de problemas globales mediante un grupo distribuido de entidades que trabajaban coordinadamente hacia objetivos comunes. Este enfoque requería intercambio mutuo de información constante para lograr un rendimiento colaborativo efectivo, con las entidades distribuidas tanto lógica como geográficamente. En contraste, el DzAI se centraba en la actividad de agentes completamente autónomos operando en un entorno multi-agente, donde cada agente actuaba racionalmente e intencionalmente respecto a sus propios objetivos específicos.\nEsta diferenciación temprana es particularmente relevante porque el concepto moderno de IA descentralizada retoma elementos conceptuales del DzAI original: entidades autónomas que persiguen sus propios incentivos, aunque ahora mediados por mecanismos económicos y blockchain que no existían en 1990.\nLa verdadera transformación del concepto llegó así con la integración de tecnologías blockchain y la visión de democratización de la inteligencia artificial. Montes & Goertzel (2019) presentaron una crítica fundamental al estado actual de la IA, argumentando que la dominación por parte de un oligopolio de mega-corporaciones centralizadas crea un terreno de juego inequitativo con implicaciones potencialmente negativas para la humanidad. Su propuesta de un sistema “distributed, decentralized, and democratized” articuló una visión comprehensiva de cómo la tecnología de ledger distribuido podría transformar fundamentalmente el desarrollo y acceso a servicios de IA. Los autores identificaron problemas sistémicos en el modelo centralizado actual: la necesidad universal de servicios de IA por parte de empresas que carecen del capital necesario para desarrollar sus propios sistemas, y la falta de visibilidad y fuentes de ingresos para desarrolladores independientes de IA. Esta asimetría en el acceso a recursos y oportunidades no solo limita la innovación, sino que también concentra el poder tecnológico en pocas manos, creando dependencias peligrosas y limitando la diversidad de enfoques y aplicaciones. Ya en 2019 los autores comentaban:\n\nArtificial intelligence is a rapidly growing industry with widespread predictions of dramatically changing the economic and labor landscape of the world. By 2020, the global AI market is projected at $47 billion (USD) and the global big data analytics market at $203 billion. To date, the overwhelming majority of AI development is done by a handful of technology mega-corporations (e.g. Facebook, Google, Amazon, IBM, Microsoft, Baidu, etc.). While the world’s population is over 7 billion people, only around 10,000 people in roughly seven countries are writing the code for all of AI (Shen, 2017). By remaining in the hands of a few, the trajectory of AI applications may be significantly compromised. The datasets used to develop such AI and the AIs themselves are biased and may not be generalizable to the wider population, and the companies are beholden to their stakeholder’s interests. The result is a ‘technocracy’ in which the future of one of the most potent set of technologies in the history of humankind is spoken for by a small biased minority (Montes & Goertzel, 2019, p. 1).\n\nHarris & Waggoner (2019), por su parte, materializaron estas ideas teóricas en un framework práctico y funcional para “Decentralized and Collaborative AI on Blockchain”. Su propuesta permitía que múltiples participantes colaboraran para construir datasets de forma distribuida y compartir modelos continuamente actualizados en una blockchain pública mediante contratos inteligentes. Este framework no solo democratizaba el acceso a modelos de IA, sino que también creaba incentivos económicos para la participación y mantenimiento de la calidad de los datos. El sistema propuesto incluía mecanismos sofisticados de validación descentralizada, donde los participantes podían verificar y validar contribuciones de datos sin necesidad de una autoridad central. Esto representaba un avance significativo hacia la autogobernanza de sistemas de IA, donde la calidad y integridad se mantienen a través de incentivos económicos y mecanismos de consenso distribuido.\nSin embargo, el trabajo también identificó limitaciones importantes en la descentralización completa. Algunos mecanismos de incentivos requerían autoridades externas que proporcionaran conjuntos de datos de prueba y fondos de recompensa, mientras que la validación frecuentemente dependía del modelo existente más que de consenso distribuido puro. Además, las restricciones técnicas de Ethereum, particularmente los altos costos de gas, limitaban la escalabilidad del sistema a modelos simples y datos pequeños, principalmente texto en lugar de aplicaciones complejas como procesamiento de imágenes. El sistema también enfrentaba vulnerabilidades ante agentes maliciosos con recursos suficientes y problemas de datos ambiguos que podían comprometer la calidad del modelo. No obstante, proporcionaron una implementación de código abierto completa que demostró la viabilidad práctica del concepto y estableció las bases para futuras investigaciones en IA colaborativa descentralizada.\nPor otra parte, en Cao (2022) se puede ver como se consolidó y expandió significativamente el campo al proporcionar un marco conceptual exhaustivo que situaba la IA descentralizada como un paradigma distinto que trasciende tanto la IA centralizada como la distribuida tradicional (DAI). Su publicación integró el concepto emergente de edge intelligence y exploró las intersecciones sinérgicas con tecnologías transformadoras como blockchain inteligente, Web3, metaverso y ciencia descentralizada (DeSci).\nEsta consolidación teórica fue crucial porque estableció la IA descentralizada no como una simple variación técnica de enfoques existentes, sino como un paradigma fundamentalmente nuevo que requiere consideraciones distintas en términos de arquitectura, gobernanza, y filosofía de desarrollo. Cao (2022) argumentó que la complementariedad y metasíntesis entre enfoques centralizados y descentralizados podría desbloquear un nuevo potencial para el desarrollo de sistemas de IA más robustos, equitativos y beneficiosos. El trabajo también exploró cómo la IA descentralizada podría habilitar y promover ecosistemas tecnológicos emergentes desde perspectivas disciplinarias, técnicas, prácticas y más amplias. Esta visión holística reconocía que la descentralización de la IA no es meramente un cambio técnico, sino una transformación que impacta aspectos económicos, sociales y éticos del desarrollo tecnológico.\nPara ilustrar la complejidad arquitectónica de este nuevo paradigma, los autores presentaron un mapa conceptual que sintetiza las principales configuraciones de la IA descentralizada. La Figura 1 muestra cinco arquitecturas fundamentales que operan en diferentes niveles de la infraestructura de red: desde sistemas P2P completamente descentralizados en dispositivos finales, hasta configuraciones híbridas que integran dispositivos, edge computing y cloud computing. Esta taxonomía arquitectónica demuestra cómo la IA descentralizada puede adaptarse a diferentes contextos operacionales, ya sea mediante el intercambio de datos, el intercambio de modelos, o configuraciones híbridas que optimizan tanto la privacidad como la eficiencia computacional.\n\n\n\n\n\n\nFigura 1: IA descentralizada: sintetizando IA en dispositivo, edge AI, y cloud AI. Ⓐ IA P2P en dispositivo. Ⓑ Edge AI con intercambio de datos. Ⓒ Edge AI con intercambio de modelos. Ⓓ Edge AI híbrida dispositivo-edge-cloud. Cao (2022)\n\n\n\nLas cinco arquitecturas ilustradas en la Figura 1 permiten diferentes grados de descentralización y especialización funcional, desde redes completamente distribuidas donde los dispositivos se conectan directamente entre sí, hasta sistemas híbridos que aprovechan las ventajas de cada nivel de la infraestructura de red para optimizar el rendimiento y la privacidad de las aplicaciones de IA.\nFinalmente, para consolidar la distinción conceptual entre ambos paradigmas, los autores presentaron una comparación sistemática que abarca los aspectos fundamentales de cada enfoque. La Figura 2 compara las propiedades, características, ventajas y desventajas de la IA centralizada (CeAI) y la IA descentralizada (DeAI) en términos de los aspectos principales de la IA: metodología, objetivos, inteligencia, tareas, datos y recursos, modelos, arquitecturas, procesos, mecanismos, computación, comunicación, toma de decisiones, salida, privacidad y seguridad. Esta comparación también demuestra la necesidad de integrar y equilibrar CeAI y DeAI en sistemas de IA complejos y resolución inteligente de problemas.\n\n\n\n\n\n\nFigura 2: Comparación entre IA centralizada (CeAI) y IA descentralizada (DeAI). Cao (2022)\n\n\n\nLa Figura 2 revela que, más allá de representar paradigmas opuestos, CeAI y DeAI exhiben características complementarias que sugieren la necesidad de enfoques híbridos para sistemas de IA complejos. Esta complementariedad arquitectónica y funcional establece las bases teóricas para el desarrollo de sistemas que puedan aprovechar las fortalezas de ambos paradigmas según los requisitos específicos de cada aplicación.\n\n\nEstado actual, características e implementaciones de inteligencia artificial descentralizada (DeAI)\nEl campo de la IA descentralizada ha experimentado una evolución conceptual significativa que requiere un análisis sistemático de sus componentes fundamentales. Keršič & Turkanović (2025) realizaron una revisión sistemática de la literatura que analizó 71 estudios para identificar los bloques constructivos esenciales de los sistemas DeAI. Su investigación adoptó un enfoque bottom-up3, categorizando los componentes técnicos fundamentales que constituyen las redes y soluciones de IA descentralizada. El estudio de Keršič y Turkanović distingue claramente entre los paradigmas históricos de inteligencia artificial distribuida y los enfoques modernos de descentralización. Mientras que el Distributed AI (DAI) tradicional y los Multi-Agent Systems (MAS) se desarrollaron desde los años 70 para distribuir computación pero manteniendo control centralizado, la IA descentralizada moderna incorpora principios Web3 fundamentales: descentralización genuina, auto-soberanía, control de datos por parte de usuarios, ausencia de gestión central y privacidad nativa. Los bloques constructivos identificados incluyen componentes de red descentralizada, mecanismos de consenso adaptados para workloads4 de IA, sistemas de incentivos económicos, protocolos de interoperabilidad y arquitecturas de edge intelligence5. Estos componentes trascienden las limitaciones de sistemas centralizados al eliminar puntos únicos de control y crear ecosistemas donde la innovación puede emerger orgánicamente de comunidades diversas.\nLa integración de edge intelligence representa uno de los avances más significativos en la arquitectura de IA descentralizada. Keršič & Turkanović (2025) identifican que este paradigma permite procesamiento local de IA cerca de las fuentes de datos, reduciendo latencia, mejorando privacidad y disminuyendo dependencias de infraestructuras centralizadas. Esta distribución geográfica del poder computacional no es meramente una optimización técnica, sino un cambio fundamental que habilita aplicaciones previamente impracticables. La edge intelligence se convierte en componente esencial para aplicaciones como procesamiento de IA en tiempo real en dispositivos IoT, vehículos autónomos con capacidades de decisión local y sistemas móviles que operan independientemente de conectividad constante. Esta arquitectura distribuida crea resiliencia sistémica donde la red puede continuar funcionando incluso cuando nodos individuales fallan o son comprometidos.\nLos sistemas de IA descentralizada implementan mecanismos de gobernanza donde las decisiones críticas se toman colectivamente a través de procesos de consenso distribuido. Harris & Waggoner (2019) demostraron cómo estos mecanismos pueden funcionar en la práctica, con participantes validando contribuciones de datos y actualizaciones de modelos sin requerir autoridades centrales. Estos sistemas de gobernanza distribuida van más allá de la simple votación; incorporan incentivos económicos sofisticados que alinean los intereses individuales con el bienestar colectivo del sistema. Los mecanismos de Self-Assessment validados por Harris (2020) muestran cómo la calidad puede emerger orgánicamente de la participación incentivizada, creando sistemas autoregulados que mejoran con el tiempo.\nOtra característica fundamental es el mantenimiento del control de datos en manos de sus creadores originales, contrastando dramáticamente con modelos centralizados donde corporaciones acumulan vastos datasets propietarios. Montes & Goertzel (2019) identifican esto como crucial para abordar preocupaciones sobre privacidad, monopolización de información y equidad en el acceso a recursos de datos. Este enfoque de soberanía de datos permite nuevos modelos de colaboración donde los participantes pueden contribuir a sistemas de IA sin sacrificar control sobre su información personal o propietaria. Los datos pueden permanecer localmente mientras contribuyen a modelos globales a través de técnicas como federated learning y privacy-preserving computation.\nEn Harris (2020) se puede abordar una validación empírica comprehensiva de sistemas de IA descentralizada en blockchain, evaluando múltiples modelos de machine learning (Perceptron, Naive Bayes, y Nearest Centroid Classifier) implementados en contratos inteligentes de Ethereum. Su investigación abordó desafíos prácticos críticos que habían sido teorizados pero no probados empíricamente. El estudio evaluó el mecanismo de incentivos Self-Assessment a través de simulaciones extensas usando tres datasets diversos: predicción de actividades deportivas con datos de Endomondo, análisis de sentimientos en reseñas de películas de IMDB y detección de noticias falsas. Esta diversidad de aplicaciones demostró la versatilidad y robustez del enfoque descentralizado across diferentes dominios y tipos de datos. Los resultados empíricos fueron reveladores: los participantes buenos que contribuían datos correctos consistentemente lograban obtener ganancias económicas, mientras que actores maliciosos que intentaban corromper los modelos perdían sus depósitos. Esto demostró que es posible crear sistemas económicamente sostenibles donde la calidad de datos se mantiene a través de incentivos alineados, sin requerir supervisión centralizada. Particularmente significativo fue el hallazgo de que diferentes modelos de machine learning tienen características de costo y rendimiento distintas en entornos blockchain. El modelo Perceptron demostró ser consistentemente el más económico en términos de costos de gas de Ethereum, mientras que modelos más complejos como Naive Bayes requerían consideraciones especiales de optimización para ser viables económicamente.\nSin embargo, investigaciones recientes también han revelado discrepancias significativas entre las promesas teóricas y las realidades de implementación. Mafrur (2025) presenta una evaluación crítica de tokens basados en IA, cuestionando si los proyectos actuales logran verdadera descentralización o simplemente crean una ilusión de descentralización mediante narrativas especulativas. El análisis de Mafrur examina proyectos líderes como RENDER, Bittensor, Fetch.ai, SingularityNET y Ocean Protocol, identificando limitaciones fundamentales en sus arquitecturas técnicas y modelos de negocio. Desde una perspectiva técnica, muchas plataformas dependen extensivamente de computación off-chain, exhiben capacidades limitadas para inteligencia on-chain y enfrentan desafíos significativos de escalabilidad. Desde una perspectiva de negocio, muchos modelos aparentemente replican estructuras de servicios de IA centralizados, simplemente añadiendo capas de pago y governance basadas en tokens sin entregar valor genuinamente novedoso.\nEsta evaluación crítica revela una brecha fundamental entre las aspiraciones de descentralización y las implementaciones actuales. Muchos proyectos que se promocionan como IA descentralizada mantienen componentes centralizados críticos, especialmente en las capas de computación y almacenamiento de modelos. La dependencia de infraestructura off-chain socava las garantías de resistencia a censura y verificabilidad que constituyen principios fundamentales de la descentralización genuina.\nLos desafíos identificados por estas investigaciones recientes van más allá de problemas técnicos superficiales. Keršič & Turkanović (2025) destacan problemas fundamentales relacionados con privacidad digital, ownership de datos y control que persisten incluso en sistemas ostensiblemente descentralizados. La transferencia de tecnologías de IA desde el ámbito académico hacia aplicaciones del mundo real acelera cada año, pero durante esta transición emergen cuestiones éticas críticas que los enfoques centralizados actuales no pueden abordar adecuadamente. La investigación revela que muchos sistemas enfrentan el trilema de descentralización en IA (Mssassi & Abou El Kalam, 2025): es extremadamente difícil lograr simultáneamente descentralización genuina, performance competitivo y escalabilidad económica. Los trade-offs entre estos objetivos a menudo resultan en compromisos que sacrifican aspectos de descentralización para mantener viabilidad práctica. Mafrur (2025) identifica que los desafíos de escalabilidad son particularmente altos porque las workloads de IA requieren recursos computacionales intensivos que son costosos de distribuir eficientemente. Los mecanismos de consenso tradicionales de blockchain no están optimizados para validar outputs de IA y los intentos de adaptación a menudo introducen centralizaciones ocultas o vulnerabilidades de seguridad.\nEl camino hacia IA verdaderamente descentralizada requerirá probablemente enfoques híbridos que combinen beneficios de centralización (eficiencia, performance) con garantías de descentralización (resistencia a censura, control de usuarios) de manera más sofisticada que las implementaciones actuales. Esto podría incluir arquitecturas de múltiples capas donde diferentes aspectos del sistema operen con diferentes grados de descentralización según sus requisitos específicos. La evaluación crítica presentada por Mafrur (2025) no descarta el potencial de la IA descentralizada, sino que enfatiza la necesidad de aproximaciones más fundamentadas y evaluación rigurosa. El campo debe evolucionar más allá de narrativas especulativas hacia implementaciones que demuestren valor técnico y social genuino, balanceando aspiraciones de descentralización con viabilidad práctica y beneficios tangibles para usuarios."
  },
  {
    "objectID": "posts/ia-descentralizada/index.html#conclusiones",
    "href": "posts/ia-descentralizada/index.html#conclusiones",
    "title": "Inteligencia artificial descentralizada (DeAI): perspectivas de un desarrollo en ciernes",
    "section": "Conclusiones",
    "text": "Conclusiones\nLa inteligencia artificial descentralizada (DeAI) representa una frontera emergente con potencial transformador. Aunque enfrenta desafíos técnicos significativos, los avances en computación distribuida, aprendizaje federado y sistemas de incentivos económicos están creando las condiciones para su viabilidad práctica.\nEl desarrollo de este paradigma no solo tiene implicaciones técnicas, sino que también plantea cuestiones fundamentales sobre el poder, la gobernanza y la equidad en la era de la inteligencia artificial. A medida que estas tecnologías maduren, es probable que veamos una coexistencia entre sistemas centralizados y descentralizados, cada uno optimizado para diferentes casos de uso y valores.\nSin embargo, el análisis presentado revela que el problema fundamental no reside en la inteligencia artificial per se, sino en los modelos de centralización que la sustentan. La concentración de datos en manos de pocas corporaciones tecnológicas, la dependencia de infraestructuras centralizadas para el procesamiento de modelos y la falta de control de los usuarios sobre sus propios datos constituyen los verdaderos desafíos que la IA descentralizada busca abordar.\nLos datos han sido últimamente el factor determinante en el desarrollo de sistemas exitosos. Como demuestran los casos analizados, desde las primeras propuestas de Montes & Goertzel (2019) hasta las implementaciones críticas evaluadas por Mafrur (2025), la cuestión central no es desarrollar algoritmos más sofisticados, sino democratizar el acceso a los datos y redistribuir el control sobre las infraestructuras que los procesan. La IA descentralizada emerge así no como una mera innovación tecnológica, sino como una respuesta necesaria a la concentración de poder digital que caracteriza la era actual.\nLa evolución hacia una IA más descentralizada no es meramente una cuestión técnica, sino una oportunidad para reimaginar cómo desarrollamos, implementamos y gobernamos las tecnologías que están redefiniendo nuestro mundo. En última instancia, el futuro de la inteligencia artificial será determinado por nuestra capacidad de resolver las tensiones entre eficiencia centralizada y autonomía descentralizada, entre innovación acelerada y control democrático. El futuro de la inteligencia artificial podría ser considerablemente más distribuido, democrático y resiliente de lo que las tendencias actuales sugieren, pero solo si logramos abordar la cuestión fundamental de quién controla los datos que alimentan estos sistemas.\n\n\n\n\n\n\nUso de IA\n\n\n\nEn la elaboración de este texto, se empleó inteligencia artificial para la redacción de párrafos, síntesis conceptual, síntesis bibliográfica, generación de metadatos y contenido en formatos MD, YAML y BibTeX. Asimismo, se utilizó para responder consultas del autor orientadas a expandir análisis críticos y perfeccionar la sintaxis. La asistencia fue proporcionada por Claude Sonnet 4 (Anthropic)."
  },
  {
    "objectID": "posts/ia-descentralizada/index.html#footnotes",
    "href": "posts/ia-descentralizada/index.html#footnotes",
    "title": "Inteligencia artificial descentralizada (DeAI): perspectivas de un desarrollo en ciernes",
    "section": "Notas",
    "text": "Notas\n\n\nFlooding (inundación) es una técnica donde un nodo envía la consulta de búsqueda a todos sus vecinos conectados, que la reenvían a todos sus vecinos, propagándose como ondas en un estanque. Ofrece alta probabilidad de encontrar contenido pero consume mucho ancho de banda debido al tráfico exponencial generado.↩︎\nRandom walks (caminatas aleatorias) es una técnica donde la consulta “camina” por la red saltando de un nodo a otro de forma aleatoria, siguiendo un camino único en lugar de multiplicarse. Consume menos ancho de banda que flooding pero tiene menor probabilidad de encontrar contenido raro.↩︎\nBottom-up approach se refiere aquí a construir la comprensión de los sistemas DeAI (DEAI en el artículo) partiendo de sus componentes más básicos (bloques de construcción) para luego entender cómo se combinan en arquitecturas más complejas, en lugar de analizar sistemas completos desde el principio.↩︎\nWorkloads se refiere a las cargas de trabajo computacionales específicas de IA, como entrenamiento de modelos, inferencia y procesamiento de datos, que requieren recursos intensivos y mecanismos de validación distribuida especializados.↩︎\nLa edge intelligence es un paradigma computacional que ejecuta algoritmos de IA directamente en dispositivos ubicados en el “borde” de la red (smartphones, sensores IoT, cámaras inteligentes), cerca de donde se generan los datos, eliminando la necesidad de enviarlos a servidores centralizados para su procesamiento.↩︎"
  }
]